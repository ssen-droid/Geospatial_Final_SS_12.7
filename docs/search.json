[
  {
    "objectID": "Final_Project_Sam_Sen.html",
    "href": "Final_Project_Sam_Sen.html",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "",
    "text": "How is search intensity for AI (specifically ChatGPT) associated with income, education, broadband access, and industry composition at the state level?\nThis analysis examines correlations between: - ChatGPT search intensity (Google Trends) - Median household income - Education levels (% with bachelor’s degree or higher) - Internet access (% households with internet) - Knowledge economy employment (% in knowledge industries)"
  },
  {
    "objectID": "Final_Project_Sam_Sen.html#research-question",
    "href": "Final_Project_Sam_Sen.html#research-question",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "",
    "text": "How is search intensity for AI (specifically ChatGPT) associated with income, education, broadband access, and industry composition at the state level?\nThis analysis examines correlations between: - ChatGPT search intensity (Google Trends) - Median household income - Education levels (% with bachelor’s degree or higher) - Internet access (% households with internet) - Knowledge economy employment (% in knowledge industries)"
  },
  {
    "objectID": "Final_Project_Sam_Sen.html#setup-and-installation",
    "href": "Final_Project_Sam_Sen.html#setup-and-installation",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "1. Setup and Installation",
    "text": "1. Setup and Installation\n\n1.1 Install Required Packages\nRun this cell first if you get ModuleNotFoundError\n\n\n1.2 Import Libraries\n\n\nCode\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\nfrom pathlib import Path\nfrom census import Census\n\n# Configure Plotly for Quarto/HTML rendering\npio.renderers.default = \"plotly_mimetype+notebook\"\n\n# Set up paths\nPROJECT_ROOT = Path.cwd()\nDATA_RAW = PROJECT_ROOT / 'Data' / 'raw'\nDATA_PROCESSED = PROJECT_ROOT / 'Data' / 'processed'\n\n# Create directories if they don't exist\nDATA_RAW.mkdir(parents=True, exist_ok=True)\nDATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n\nprint(\"✓ Libraries imported\")\nprint(f\"✓ Project root: {PROJECT_ROOT}\")\nprint(f\"✓ Data raw folder: {DATA_RAW}\")\nprint(f\"✓ Data processed folder: {DATA_PROCESSED}\")\n\n\n✓ Libraries imported\n✓ Project root: c:\\Users\\16468\\OneDrive - PennO365\\Documents\\Academics\\MUSA\\MUSA_5500\\Final Project\n✓ Data raw folder: c:\\Users\\16468\\OneDrive - PennO365\\Documents\\Academics\\MUSA\\MUSA_5500\\Final Project\\Data\\raw\n✓ Data processed folder: c:\\Users\\16468\\OneDrive - PennO365\\Documents\\Academics\\MUSA\\MUSA_5500\\Final Project\\Data\\processed"
  },
  {
    "objectID": "Final_Project_Sam_Sen.html#load-google-trends-data",
    "href": "Final_Project_Sam_Sen.html#load-google-trends-data",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "2. Load Google Trends Data",
    "text": "2. Load Google Trends Data\n\n2.1 Load ChatGPT Search Intensity Data\nInstructions: 1. Go to Google Trends 2. Search for “ChatGPT” 3. Set time range: Past 12 months 4. Set geography: United States → Subregion (states) 5. Download CSV 6. Save the CSV file as chatgpt_googletrends.csv in the Data/raw/ folder\n\n\nCode\n# Load Google Trends data for ChatGPT\ntrends_file = DATA_RAW / 'chatgpt_googletrends.csv'\n\nif trends_file.exists():\n    # Read CSV - header is in first row\n    trends_raw = pd.read_csv(trends_file)\n    \n    # The first row contains the actual column names, so use it as header\n    trends_raw.columns = trends_raw.iloc[0]\n    # Drop the first row (it was the header)\n    trends_raw = trends_raw.drop(trends_raw.index[0]).reset_index(drop=True)\n    \n    print(f\"✓ Loaded Google Trends data from: {trends_file}\")\n    print(f\"\\nChatGPT columns: {trends_raw.columns.tolist()}\")\n    print(f\"\\nChatGPT first few rows:\")\n    print(trends_raw.head())\n    \n    # Identify the column with state names and the column with interest values\n    state_col = None\n    interest_col = None\n    \n    for col in trends_raw.columns:\n        if 'region' in str(col).lower() or 'geo' in str(col).lower():\n            state_col = col\n        if 'chatgpt' in str(col).lower() or 'interest' in str(col).lower():\n            interest_col = col\n    \n    if state_col and interest_col:\n        print(f\"\\n✓ Using columns: '{state_col}' for State, '{interest_col}' for Interest\")\n        trends_df = trends_raw[[state_col, interest_col]].copy()\n        trends_df.columns = ['State', 'Avg_AI_Interest']\n        \n        # Remove any rows with missing data\n        trends_df = trends_df.dropna()\n        \n        print(f\"\\nTrends data shape: {trends_df.shape}\")\n        print(f\"\\nFirst few rows:\")\n        print(trends_df.head())\n        print(f\"\\nSummary statistics:\")\n        print(trends_df['Avg_AI_Interest'].describe())\n        print(f\"\\nNumber of states: {len(trends_df)}\")\n    else:\n        print(\"⚠ Could not identify state and interest columns automatically.\")\n        print(\"Available columns:\", trends_raw.columns.tolist())\n        trends_df = pd.DataFrame()\nelse:\n    print(f\"⚠ Google Trends file not found at: {trends_file}\")\n    trends_df = pd.DataFrame()\n\n\n✓ Loaded Google Trends data from: c:\\Users\\16468\\OneDrive - PennO365\\Documents\\Academics\\MUSA\\MUSA_5500\\Final Project\\Data\\raw\\chatgpt_googletrends.csv\n\nChatGPT columns: [nan, nan]\n\nChatGPT first few rows:\n0                   NaN                           NaN\n0                Region  ChatGPT: (12/2/24 - 12/2/25)\n1  District of Columbia                           100\n2            California                            98\n3               Georgia                            92\n4            New Jersey                            90\n⚠ Could not identify state and interest columns automatically.\nAvailable columns: [nan, nan]"
  },
  {
    "objectID": "Final_Project_Sam_Sen.html#census-api-setup",
    "href": "Final_Project_Sam_Sen.html#census-api-setup",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "3. Census API Setup",
    "text": "3. Census API Setup\n\n3.1 Initialize Census API\n\n\nCode\n# Load Census API key from config file\ntry:\n    from config import CENSUS_API_KEY\n    c = Census(CENSUS_API_KEY, year=2022)  # Using 2022 ACS 5-year estimates\n    print(\"Census API initialized\")\nexcept ImportError:\n    print(\"⚠ Error: config.py not found\")\n    print(\"Please create config.py with your Census API key:\")\n    print(\"  CENSUS_API_KEY = 'your_api_key_here'\")\n    c = None\nexcept Exception as e:\n    print(f\"⚠ Error initializing Census API: {e}\")\n    c = None\n\n\nCensus API initialized"
  },
  {
    "objectID": "Final_Project_Sam_Sen.html#collect-acs-data",
    "href": "Final_Project_Sam_Sen.html#collect-acs-data",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "4. Collect ACS Data",
    "text": "4. Collect ACS Data\n\n4.1 Broadband Access (Table B28002)\n\n\nCode\n# ACS Table B28002: Internet Subscription by Type\n# Using simpler approach: Total with internet subscription (any type)\n# Variables:\n# B28002_001E: Total households\n# B28002_002E: With internet subscription (any type - includes broadband, cellular, satellite, etc.)\n\nif c:\n    broadband_data = c.acs5.state(\n        ('NAME', 'B28002_001E', 'B28002_002E'),\n        Census.ALL\n    )\n    \n    broadband_df = pd.DataFrame(broadband_data)\n    \n    # Convert to numeric\n    numeric_cols = ['B28002_001E', 'B28002_002E']\n    for col in numeric_cols:\n        broadband_df[col] = pd.to_numeric(broadband_df[col], errors='coerce')\n    \n    # Calculate percentages\n    # pct_with_internet: % of households with any internet subscription (our main measure)\n    # pct_no_internet: % of households with no internet subscription (calculated as Total - With Internet)\n    broadband_df['pct_with_internet'] = (broadband_df['B28002_002E'] / broadband_df['B28002_001E']) * 100\n    broadband_df['pct_no_internet'] = ((broadband_df['B28002_001E'] - broadband_df['B28002_002E']) / broadband_df['B28002_001E']) * 100\n    \n    # Rename state column\n    broadband_df = broadband_df.rename(columns={'NAME': 'State'})\n    \n    # Remove 'United States' row if present\n    broadband_df = broadband_df[broadband_df['State'] != 'United States']\n    \n    print(f\"Broadband data shape: {broadband_df.shape}\")\n    print(\"\\nFirst 5 states:\")\n    print(broadband_df[['State', 'B28002_001E', 'B28002_002E', 'pct_with_internet', 'pct_no_internet']].head())\n    print(\"\\nVerification (should add to 100%):\")\n    broadband_df['check_sum'] = broadband_df['pct_with_internet'] + broadband_df['pct_no_internet']\n    print(broadband_df[['State', 'pct_with_internet', 'pct_no_internet', 'check_sum']].head())\n    print(\"\\nSummary statistics:\")\n    print(broadband_df[['pct_with_internet', 'pct_no_internet']].describe())\n    print(\"\\nNote: pct_with_internet includes all types of internet (broadband, cellular, satellite, etc.)\")\n    print(\"      pct_no_internet is calculated as: (Total - With Internet) / Total\")\nelse:\n    print(\"⚠ Census API not initialized. Cannot fetch broadband data.\")\n    broadband_df = pd.DataFrame()\n\n\nBroadband data shape: (52, 6)\n\nFirst 5 states:\n        State  B28002_001E  B28002_002E  pct_with_internet  pct_no_internet\n0     Alabama    1933150.0    1625807.0          84.101441        15.898559\n1      Alaska     264376.0     236875.0          89.597770        10.402230\n2     Arizona    2739136.0    2448838.0          89.401841        10.598159\n3    Arkansas    1171694.0     968272.0          82.638641        17.361359\n4  California   13315822.0   12195945.0          91.589877         8.410123\n\nVerification (should add to 100%):\n        State  pct_with_internet  pct_no_internet  check_sum\n0     Alabama          84.101441        15.898559      100.0\n1      Alaska          89.597770        10.402230      100.0\n2     Arizona          89.401841        10.598159      100.0\n3    Arkansas          82.638641        17.361359      100.0\n4  California          91.589877         8.410123      100.0\n\nSummary statistics:\n       pct_with_internet  pct_no_internet\ncount          52.000000        52.000000\nmean           87.693465        12.306535\nstd             3.369030         3.369030\nmin            73.186992         7.724270\n25%            86.687535        10.018033\n50%            88.242682        11.757318\n75%            89.981967        13.312465\nmax            92.275730        26.813008\n\nNote: pct_with_internet includes all types of internet (broadband, cellular, satellite, etc.)\n      pct_no_internet is calculated as: (Total - With Internet) / Total\n\n\n\n\n4.2 Education Level (Table B15003)\n\n\nCode\n# ACS Table B15003: Educational Attainment\n# Calculate % of adults (25+) with bachelor's degree or higher\n# Variables:\n# B15003_001E: Total (population 25 years and over)\n# B15003_022E through B15003_025E: Bachelor's, Master's, Professional, Doctorate degrees\n\nif c:\n    education_data = c.acs5.state(\n        ('NAME', 'B15003_001E', 'B15003_022E', 'B15003_023E', 'B15003_024E', 'B15003_025E'),\n        Census.ALL\n    )\n    \n    education_df = pd.DataFrame(education_data)\n    \n    # Convert to numeric\n    numeric_cols = ['B15003_001E', 'B15003_022E', 'B15003_023E', 'B15003_024E', 'B15003_025E']\n    for col in numeric_cols:\n        education_df[col] = pd.to_numeric(education_df[col], errors='coerce')\n    \n    # Calculate % with bachelor's degree or higher\n    # Sum: Bachelor's + Master's + Professional + Doctorate\n    education_df['bachelors_plus'] = (\n        education_df['B15003_022E'] +  # Bachelor's degree\n        education_df['B15003_023E'] +  # Master's degree\n        education_df['B15003_024E'] +  # Professional degree\n        education_df['B15003_025E']    # Doctorate degree\n    )\n    \n    education_df['pct_bachelors_plus'] = (education_df['bachelors_plus'] / education_df['B15003_001E']) * 100\n    \n    # Rename state column\n    education_df = education_df.rename(columns={'NAME': 'State'})\n    \n    # Remove 'United States' row if present\n    education_df = education_df[education_df['State'] != 'United States']\n    \n    print(f\"Education data shape: {education_df.shape}\")\n    print(\"\\nFirst 5 states:\")\n    print(education_df[['State', 'pct_bachelors_plus']].head())\n    print(\"\\nSummary statistics:\")\n    print(education_df['pct_bachelors_plus'].describe())\nelse:\n    print(\"⚠ Census API not initialized. Cannot fetch education data.\")\n    education_df = pd.DataFrame()\n\n\nEducation data shape: (52, 9)\n\nFirst 5 states:\n        State  pct_bachelors_plus\n0     Alabama           27.208387\n1      Alaska           30.748285\n2     Arizona           31.798873\n3    Arkansas           24.705551\n4  California           35.864402\n\nSummary statistics:\ncount    52.000000\nmean     33.767735\nstd       6.697265\nmin      22.711161\n25%      30.083528\n50%      32.898278\n75%      36.378206\nmax      62.636093\nName: pct_bachelors_plus, dtype: float64\n\n\n\n\n4.3 Median Household Income (Table B19013)\n\n\nCode\n# ACS Table B19013: Median Household Income\n# Variable: B19013_001E: Median household income in the past 12 months\n\nif c:\n    income_data = c.acs5.state(\n        ('NAME', 'B19013_001E'),\n        Census.ALL\n    )\n    \n    income_df = pd.DataFrame(income_data)\n    \n    # Convert to numeric\n    income_df['B19013_001E'] = pd.to_numeric(income_df['B19013_001E'], errors='coerce')\n    \n    # Rename columns\n    income_df = income_df.rename(columns={\n        'NAME': 'State',\n        'B19013_001E': 'median_household_income'\n    })\n    \n    # Remove 'United States' row if present\n    income_df = income_df[income_df['State'] != 'United States']\n    \n    print(f\"Income data shape: {income_df.shape}\")\n    print(\"\\nFirst 5 states:\")\n    print(income_df.head())\n    print(\"\\nSummary statistics:\")\n    print(income_df['median_household_income'].describe())\nelse:\n    print(\"⚠ Census API not initialized. Cannot fetch income data.\")\n    income_df = pd.DataFrame()\n\n\nIncome data shape: (52, 3)\n\nFirst 5 states:\n        State  median_household_income state\n0     Alabama                  59609.0    01\n1      Alaska                  86370.0    02\n2     Arizona                  72581.0    04\n3    Arkansas                  56335.0    05\n4  California                  91905.0    06\n\nSummary statistics:\ncount        52.000000\nmean      73828.057692\nstd       14182.829754\nmin       24002.000000\n25%       66302.250000\n50%       72090.000000\n75%       84827.250000\nmax      101722.000000\nName: median_household_income, dtype: float64\n\n\n\n\n4.4 Industry Composition (BLS Data)\nLoad manually downloaded BLS industry data with % Knowledge Economy column.\n\n\nCode\n# Load BLS Industry Data - Using your manually calculated %knowledge economy column\n# The file should have a State column and a %knowledge economy column (rightmost column)\n\ntry:\n    # Try CSV first (if you saved as CSV)\n    csv_file = DATA_RAW / 'BLS_Industry_Data.csv'\n    excel_file = DATA_RAW / 'BLS_Industry_Data.xlsx'\n    \n    if csv_file.exists():\n        bls_data = pd.read_csv(csv_file)\n        print(f\"✓ Loaded BLS data from CSV: {csv_file}\")\n    elif excel_file.exists():\n        bls_data = pd.read_excel(excel_file)\n        print(f\"✓ Loaded BLS data from Excel: {excel_file}\")\n    else:\n        print(f\"⚠ BLS file not found. Looking for:\")\n        print(f\"  - {csv_file}\")\n        print(f\"  - {excel_file}\")\n        bls_data = pd.DataFrame()\n    \n    if not bls_data.empty:\n        print(f\"\\nFile shape: {bls_data.shape}\")\n        print(f\"Columns: {bls_data.columns.tolist()}\")\n        print(f\"\\nFirst few rows:\")\n        print(bls_data.head())\n        \n        # Find the State column\n        state_col = None\n        for col in bls_data.columns:\n            if 'state' in str(col).lower() and 'code' not in str(col).lower():\n                state_col = col\n                break\n        \n        # Use the rightmost column for %knowledge economy (the one you added)\n        knowledge_col = bls_data.columns[-1]\n        print(f\"\\n✓ Using rightmost column for knowledge economy: '{knowledge_col}'\")\n        \n        if state_col:\n            # Create final dataset\n            industry_final = bls_data[[state_col, knowledge_col]].copy()\n            industry_final.columns = ['State', 'pct_knowledge']\n            \n            # Clean state names if needed\n            industry_final['State'] = industry_final['State'].str.strip()\n            \n            # Convert pct_knowledge to numeric\n            industry_final['pct_knowledge'] = pd.to_numeric(industry_final['pct_knowledge'], errors='coerce')\n            \n            # Remove any rows with missing data\n            industry_final = industry_final.dropna()\n            \n            print(f\"\\n✓ Industry data processed successfully!\")\n            print(f\"  States: {len(industry_final)}\")\n            print(f\"\\nFirst 10 states:\")\n            print(industry_final.head(10))\n            print(f\"\\nSummary statistics for % Knowledge Economy:\")\n            print(industry_final['pct_knowledge'].describe().round(2))\n        else:\n            print(f\"\\n⚠ Could not find State column.\")\n            industry_final = pd.DataFrame()\n    \nexcept Exception as e:\n    print(f\"Error loading BLS data: {e}\")\n    import traceback\n    traceback.print_exc()\n    industry_final = pd.DataFrame()\n\n\n✓ Loaded BLS data from Excel: c:\\Users\\16468\\OneDrive - PennO365\\Documents\\Academics\\MUSA\\MUSA_5500\\Final Project\\Data\\raw\\BLS_Industry_Data.xlsx\n\nFile shape: (52, 17)\nColumns: ['State', 'Total nonfarm', 'Mining and logging', 'Mining, logging, and construction', 'Construction', 'Manufacturing', 'Trade, transportation, and utilities', 'Information', 'Financial activities', 'Professional and business services', 'Education and health services', 'Leisure and hospitality', 'Other services', 'Government', 'Knowledge Economy ', 'Other', '% Knowledge']\n\nFirst few rows:\n        State  Total nonfarm  Mining and logging  \\\n0     Alabama         2214.9                 9.2   \n1      Alaska          339.0                11.8   \n2     Arizona         3253.7                16.2   \n3    Arkansas         1385.8                 5.1   \n4  California        18011.2                18.8   \n\n   Mining, logging, and construction  Construction  Manufacturing  \\\n0                              119.7         110.5          287.3   \n1                               31.6          19.8           12.5   \n2                              242.6         226.4          192.4   \n3                               70.6          65.5          164.7   \n4                              911.8         893.0         1211.2   \n\n   Trade, transportation, and utilities  Information  Financial activities  \\\n0                                 411.7         22.9                 102.6   \n1                                  68.0          4.0                  10.7   \n2                                 615.6         46.4                 241.8   \n3                                 273.9         11.7                  70.9   \n4                                3071.7        526.6                 783.4   \n\n   Professional and business services  Education and health services  \\\n0                               262.7                          272.2   \n1                                29.6                           53.5   \n2                               458.3                          560.3   \n3                               161.1                          222.9   \n4                              2726.9                         3466.1   \n\n   Leisure and hospitality  Other services Government  Knowledge Economy   \\\n0                    213.4           100.3      422.1               660.4   \n1                     35.8            12.9       80.4                97.8   \n2                    363.3           106.6      426.4              1306.8   \n3                    132.0            65.2      212.8               466.6   \n4                   2012.3           595.8     2705.4              7503.0   \n\n     Other  % Knowledge  \n0   1554.5     0.298162  \n1    241.2     0.288496  \n2   1946.9     0.401635  \n3    919.2     0.336701  \n4  10508.2     0.416574  \n\n✓ Using rightmost column for knowledge economy: '% Knowledge'\n\n✓ Industry data processed successfully!\n  States: 51\n\nFirst 10 states:\n                  State  pct_knowledge\n0               Alabama       0.298162\n1                Alaska       0.288496\n2               Arizona       0.401635\n3              Arkansas       0.336701\n4            California       0.416574\n5              Colorado       0.378873\n6           Connecticut       0.435008\n7              Delaware       0.422641\n8  District of Columbia       0.443389\n9               Florida       0.403557\n\nSummary statistics for % Knowledge Economy:\ncount    51.00\nmean      0.37\nstd       0.05\nmin       0.22\n25%       0.33\n50%       0.37\n75%       0.40\nmax       0.49\nName: pct_knowledge, dtype: float64"
  },
  {
    "objectID": "Final_Project_Sam_Sen.html#merge-all-data",
    "href": "Final_Project_Sam_Sen.html#merge-all-data",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "5. Merge all data",
    "text": "5. Merge all data\n\n\nCode\n# Merge All Data Together\n# Combine Google Trends, Broadband, Education, Income, and Industry data\n\n# Check if merged file already exists\nmerged_file = DATA_PROCESSED / 'merged_state_data.csv'\n\nif merged_file.exists():\n    print(\"✓ Found existing merged data file. Loading from disk...\")\n    merged_df = pd.read_csv(merged_file)\n    \n    # Convert pct_knowledge from decimal (0.4) to percentage (40) if needed\n    if 'pct_knowledge' in merged_df.columns:\n        # Check if values are in decimal format (max &lt; 1) and convert if needed\n        if merged_df['pct_knowledge'].max() &lt; 1:\n            print(\"  Converting pct_knowledge from decimal to percentage format...\")\n            merged_df['pct_knowledge'] = merged_df['pct_knowledge'] * 100\n    \n    print(f\"✓ Loaded merged data from: {merged_file}\")\n    print(f\"  Shape: {merged_df.shape}\")\n    print(f\"  Columns: {merged_df.columns.tolist()}\")\n    print(f\"  Number of states: {len(merged_df)}\")\n    print(f\"\\nFirst few rows:\")\n    print(merged_df.head())\nelse:\n    print(\"⚠ Merged file not found. Creating merged dataset from scratch...\")\n    \n    # First, ensure trends_df has a 'State' column - reload if needed\n    if 'State' not in trends_df.columns or trends_df.empty:\n        print(\"Reloading Google Trends data...\")\n        trends_file = DATA_RAW / 'chatgpt_googletrends.csv'\n        if trends_file.exists():\n            trends_raw = pd.read_csv(trends_file)\n            # Use first row as column names\n            trends_raw.columns = trends_raw.iloc[0]\n            trends_raw = trends_raw.drop(trends_raw.index[0]).reset_index(drop=True)\n            \n            # Find columns\n            state_col = None\n            interest_col = None\n            for col in trends_raw.columns:\n                if 'region' in str(col).lower():\n                    state_col = col\n                if 'chatgpt' in str(col).lower():\n                    interest_col = col\n            \n            if state_col and interest_col:\n                trends_df = trends_raw[[state_col, interest_col]].copy()\n                trends_df.columns = ['State', 'Avg_AI_Interest']\n                trends_df = trends_df.dropna()\n                print(f\"✓ Reloaded trends_df. Shape: {trends_df.shape}\")\n            else:\n                print(\"ERROR: Could not find columns in CSV\")\n        else:\n            print(\"ERROR: CSV file not found\")\n\n    print(f\"\\ntrends_df columns: {trends_df.columns.tolist()}\")\n    print(f\"trends_df shape: {trends_df.shape}\")\n\n    # Now merge\n    merged_df = trends_df.copy()\n\n    # Merge broadband data\n    merged_df = merged_df.merge(\n        broadband_df[['State', 'pct_with_internet', 'pct_no_internet']],\n        on='State',\n        how='inner'\n    )\n\n    # Merge education data\n    merged_df = merged_df.merge(\n        education_df[['State', 'pct_bachelors_plus']],\n        on='State',\n        how='inner'\n    )\n\n    # Merge income data\n    merged_df = merged_df.merge(\n        income_df[['State', 'median_household_income']],\n        on='State',\n        how='inner'\n    )\n\n    # Merge industry/knowledge economy data (if available)\n    if 'industry_final' in locals() and len(industry_final) &gt; 0:\n        merged_df = merged_df.merge(\n            industry_final[['State', 'pct_knowledge']],\n            on='State',\n            how='inner'\n        )\n        # Convert pct_knowledge from decimal (0.4) to percentage (40) for consistency\n        # Check if values are in decimal format (max &lt; 1) and convert if needed\n        if merged_df['pct_knowledge'].max() &lt; 1:\n            print(\"  Converting pct_knowledge from decimal to percentage format...\")\n            merged_df['pct_knowledge'] = merged_df['pct_knowledge'] * 100\n        print(\"✓ Knowledge economy data merged\")\n    else:\n        print(\"⚠ Knowledge economy data not available - skipping merge\")\n        merged_df['pct_knowledge'] = np.nan\n\n    print(f\"\\nMerged dataset shape: {merged_df.shape}\")\n    print(f\"Number of states: {len(merged_df)}\")\n    print(f\"\\nColumn names:\")\n    print(merged_df.columns.tolist())\n    print(f\"\\nFirst few rows:\")\n    print(merged_df.head())\n\n    # Save merged dataset\n    merged_df.to_csv(DATA_PROCESSED / 'merged_state_data.csv', index=False)\n    print(f\"\\n✓ Merged data saved to: {DATA_PROCESSED / 'merged_state_data.csv'}\")\n\n\n✓ Found existing merged data file. Loading from disk...\n  Converting pct_knowledge from decimal to percentage format...\n✓ Loaded merged data from: c:\\Users\\16468\\OneDrive - PennO365\\Documents\\Academics\\MUSA\\MUSA_5500\\Final Project\\Data\\processed\\merged_state_data.csv\n  Shape: (51, 7)\n  Columns: ['State', 'Avg_AI_Interest', 'pct_with_internet', 'pct_no_internet', 'pct_bachelors_plus', 'median_household_income', 'pct_knowledge']\n  Number of states: 51\n\nFirst few rows:\n                  State  Avg_AI_Interest  pct_with_internet  pct_no_internet  \\\n0  District of Columbia              100          89.118863        10.881137   \n1            California               98          91.589877         8.410123   \n2               Georgia               92          87.889289        12.110711   \n3            New Jersey               90          90.722078         9.277922   \n4                 Texas               88          88.519366        11.480634   \n\n   pct_bachelors_plus  median_household_income  pct_knowledge  \n0           62.636093                 101722.0      44.338876  \n1           35.864402                  91905.0      41.657413  \n2           33.633675                  71355.0      37.749020  \n3           42.252478                  97126.0      42.964549  \n4           32.265897                  73035.0      36.787778  \n\n\n\n\nCode\n# Add Red/Blue State Dummy Variable\n# Based on 2020 Presidential Election results\n# 1 = Blue State (Democratic), 0 = Red State (Republican)\n\n# States that voted Democratic in 2020 (Blue States)\nblue_states_2020 = {\n    'Alabama': 0, 'Alaska': 0, 'Arizona': 1, 'Arkansas': 0, 'California': 1,\n    'Colorado': 1, 'Connecticut': 1, 'Delaware': 1, 'Florida': 0, 'Georgia': 1,\n    'Hawaii': 1, 'Idaho': 0, 'Illinois': 1, 'Indiana': 0, 'Iowa': 0,\n    'Kansas': 0, 'Kentucky': 0, 'Louisiana': 0, 'Maine': 1, 'Maryland': 1,\n    'Massachusetts': 1, 'Michigan': 1, 'Minnesota': 1, 'Mississippi': 0, 'Missouri': 0,\n    'Montana': 0, 'Nebraska': 0, 'Nevada': 1, 'New Hampshire': 1, 'New Jersey': 1,\n    'New Mexico': 1, 'New York': 1, 'North Carolina': 0, 'North Dakota': 0, 'Ohio': 0,\n    'Oklahoma': 0, 'Oregon': 1, 'Pennsylvania': 1, 'Rhode Island': 1, 'South Carolina': 0,\n    'South Dakota': 0, 'Tennessee': 0, 'Texas': 0, 'Utah': 0, 'Vermont': 1,\n    'Virginia': 1, 'Washington': 1, 'West Virginia': 0, 'Wisconsin': 1, 'Wyoming': 0,\n    'District of Columbia': 1\n}\n\n# Add blue_state dummy variable (1 = Blue/Democratic, 0 = Red/Republican)\nmerged_df['blue_state'] = merged_df['State'].map(blue_states_2020)\n\n# Check for any missing values\nif merged_df['blue_state'].isna().any():\n    missing_states = merged_df[merged_df['blue_state'].isna()]['State'].tolist()\n    print(f\"⚠ Warning: Missing political classification for: {missing_states}\")\n    print(\"  These states will be excluded from regression analysis\")\nelse:\n    print(\"✓ Red/Blue state dummy variable added successfully\")\n\n# Display summary\nprint(f\"\\nPolitical Classification Summary:\")\nprint(f\"  Blue States (Democratic): {merged_df['blue_state'].sum()} states\")\nprint(f\"  Red States (Republican): {(merged_df['blue_state'] == 0).sum()} states\")\nprint(f\"\\nSample of data:\")\nprint(merged_df[['State', 'blue_state']].head(10))\n\n\n✓ Red/Blue state dummy variable added successfully\n\nPolitical Classification Summary:\n  Blue States (Democratic): 26 states\n  Red States (Republican): 25 states\n\nSample of data:\n                  State  blue_state\n0  District of Columbia           1\n1            California           1\n2               Georgia           1\n3            New Jersey           1\n4                 Texas           0\n5               Arizona           1\n6              Maryland           1\n7              Virginia           1\n8               Florida           0\n9         Massachusetts           1"
  },
  {
    "objectID": "Final_Project_Sam_Sen.html#scatterplots",
    "href": "Final_Project_Sam_Sen.html#scatterplots",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "6. Scatterplots",
    "text": "6. Scatterplots\n\n\nCode\n# Scatterplots: Each Variable vs ChatGPT Search Intensity\n# Create individual scatterplots for all predictor variables\n\n# Variables to plot against ChatGPT search intensity\nvariables = {\n    'median_household_income': 'Median Household Income ($)',\n    'pct_bachelors_plus': '% Adults with Bachelor\\'s Degree or Higher',\n    'pct_with_internet': '% Households with Internet Access',\n    'pct_knowledge': '% Knowledge Economy Employment'\n}\n\n# Filter out variables that don't exist in the dataset\navailable_vars = {k: v for k, v in variables.items() if k in merged_df.columns}\n\n# Create subplots - 2x2 grid\nn_vars = len(available_vars)\nn_cols = 2\nn_rows = (n_vars + 1) // 2\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))\naxes = axes.flatten() if n_vars &gt; 1 else [axes]\n\nfor idx, (var, label) in enumerate(available_vars.items()):\n    ax = axes[idx]\n    \n    # Remove rows with missing data for this variable\n    plot_data = merged_df[[var, 'Avg_AI_Interest']].dropna()\n    \n    if len(plot_data) &gt; 0:\n        # Scatter plot\n        ax.scatter(plot_data[var], plot_data['Avg_AI_Interest'], \n                   alpha=0.6, s=100, color='steelblue', edgecolors='black', linewidth=0.5)\n        \n        # Add regression line\n        from scipy import stats\n        slope, intercept, r_value, p_value, std_err = stats.linregress(plot_data[var], plot_data['Avg_AI_Interest'])\n        line_x = np.linspace(plot_data[var].min(), plot_data[var].max(), 100)\n        line_y = intercept + slope * line_x\n        ax.plot(line_x, line_y, 'r--', linewidth=2, \n                label=f'R² = {r_value**2:.3f}, p = {p_value:.4f}')\n        \n        ax.set_xlabel(label, fontsize=12, fontweight='bold')\n        ax.set_ylabel('ChatGPT Search Intensity', fontsize=12, fontweight='bold')\n        ax.set_title(f'{label}\\nvs ChatGPT Search Intensity', fontsize=13, fontweight='bold', pad=10)\n        ax.legend(fontsize=10)\n        ax.grid(True, alpha=0.3)\n    else:\n        ax.text(0.5, 0.5, f'No data for {label}', \n                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n        ax.set_title(label, fontsize=13, fontweight='bold')\n\n# Hide extra subplots if any\nfor idx in range(len(available_vars), len(axes)):\n    axes[idx].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n✓ Created scatterplots for {len(available_vars)} variables\")\n\n\n\n\n\n\n\n\n\n\n✓ Created scatterplots for 4 variables"
  },
  {
    "objectID": "Final_Project_Sam_Sen.html#map-visualizations",
    "href": "Final_Project_Sam_Sen.html#map-visualizations",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "7. Map Visualizations",
    "text": "7. Map Visualizations\n\n\nCode\n# Interactive Map: Switch Between All Variables\n# This interactive map allows you to explore all variables using a dropdown menu\n\nprint(\"=\" * 80)\nprint(\"INTERACTIVE MAP: Explore All Variables\")\nprint(\"=\" * 80)\nprint(\"\\nUse the dropdown menu below to switch between different variables.\")\nprint(\"Hover over states to see detailed information.\\n\")\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Add state abbreviations if not already present\nstate_abbrev = {\n    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA',\n    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA',\n    'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO',\n    'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ',\n    'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH',\n    'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT',\n    'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY',\n    'District of Columbia': 'DC'\n}\n\nif 'state_code' not in merged_df.columns:\n    merged_df['state_code'] = merged_df['State'].map(state_abbrev)\n\n# Prepare data for interactive map\ninteractive_df = merged_df.copy()\n\n# Create a list of all available variables with their display names and color scales\nvariables_config = [\n    {\n        'value': 'Avg_AI_Interest',\n        'label': 'ChatGPT Search Intensity',\n        'colorscale': 'Viridis',\n        'colorbar_title': 'Search Intensity',\n        'hover_template': '&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;Search Intensity: %{z}&lt;extra&gt;&lt;/extra&gt;'\n    },\n    {\n        'value': 'median_household_income',\n        'label': 'Median Household Income',\n        'colorscale': 'Blues',\n        'colorbar_title': 'Income ($)',\n        'hover_template': '&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;Median Income: $%{z:,.0f}&lt;extra&gt;&lt;/extra&gt;'\n    },\n    {\n        'value': 'pct_bachelors_plus',\n        'label': '% Bachelor\\'s Degree or Higher',\n        'colorscale': 'Greens',\n        'colorbar_title': '% Bachelor\\'s+',\n        'hover_template': '&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;% Bachelor\\'s+: %{z:.1f}%&lt;extra&gt;&lt;/extra&gt;'\n    },\n    {\n        'value': 'pct_with_internet',\n        'label': '% Households with Internet Access',\n        'colorscale': 'Oranges',\n        'colorbar_title': '% With Internet',\n        'hover_template': '&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;% With Internet: %{z:.1f}%&lt;extra&gt;&lt;/extra&gt;'\n    },\n    {\n        'value': 'pct_knowledge',\n        'label': '% Knowledge Economy Employment',\n        'colorscale': 'Purples',\n        'colorbar_title': '% Knowledge Economy',\n        'hover_template': '&lt;b&gt;%{customdata[0]}&lt;/b&gt;&lt;br&gt;% Knowledge Economy: %{z:.1f}%&lt;extra&gt;&lt;/extra&gt;'\n    }\n]\n\n# Create initial figure with ChatGPT Search Intensity\nfig_interactive = go.Figure()\n\n# Add all traces (one for each variable) but only show the first one initially\nfor i, var_config in enumerate(variables_config):\n    var_name = var_config['value']\n    visible = (i == 0)  # Only first variable is visible initially\n    \n    fig_interactive.add_trace(\n        go.Choropleth(\n            locations=interactive_df['state_code'],\n            z=interactive_df[var_name],\n            locationmode='USA-states',\n            colorscale=var_config['colorscale'],\n            colorbar=dict(\n                title=var_config['colorbar_title'],\n                thickness=15,\n                len=0.5,\n                x=1.02\n            ),\n            hovertemplate=var_config['hover_template'],\n            customdata=interactive_df[['State', var_name]],\n            visible=visible,\n            name=var_config['label']\n        )\n    )\n\n# Create dropdown menu\ndropdown_buttons = []\nfor i, var_config in enumerate(variables_config):\n    # Create visibility list - all False except for the selected variable\n    visibility = [False] * len(variables_config)\n    visibility[i] = True\n    \n    dropdown_buttons.append(\n        dict(\n            label=var_config['label'],\n            method='update',\n            args=[\n                {'visible': visibility},\n                {\n                    'title': f'{var_config[\"label\"]} by State',\n                    'coloraxis.colorbar.title.text': var_config['colorbar_title']\n                }\n            ]\n        )\n    )\n\n# Update layout with dropdown\nfig_interactive.update_layout(\n    title={\n        'text': 'Interactive Map: Select Variable to View&lt;br&gt;&lt;sub&gt;Use the dropdown menu to switch between variables&lt;/sub&gt;',\n        'x': 0.5,\n        'xanchor': 'center',\n        'font': {'size': 18}\n    },\n    geo=dict(\n        scope='usa',\n        projection=go.layout.geo.Projection(type='albers usa'),\n        showlakes=True,\n        lakecolor='rgb(255, 255, 255)'\n    ),\n    height=600,\n    updatemenus=[\n        dict(\n            buttons=dropdown_buttons,\n            direction='down',\n            showactive=True,\n            x=0.02,\n            xanchor='left',\n            y=1.02,\n            yanchor='top',\n            bgcolor='rgba(255, 255, 255, 0.8)',\n            bordercolor='rgba(0, 0, 0, 0.2)',\n            borderwidth=1,\n            font=dict(size=12)\n        )\n    ],\n    margin=dict(l=0, r=0, t=100, b=0)\n)\n\nfig_interactive.show()\n\nprint(\"\\n✓ Interactive map created\")\nprint(\"  - Use the dropdown menu in the top-left to switch between variables\")\nprint(\"  - Hover over states to see detailed information\")\nprint(\"=\" * 80)\n\n\n================================================================================\nINTERACTIVE MAP: Explore All Variables\n================================================================================\n\nUse the dropdown menu below to switch between different variables.\nHover over states to see detailed information.\n\n\n\n                                                    \n\n\n\n✓ Interactive map created\n  - Use the dropdown menu in the top-left to switch between variables\n  - Hover over states to see detailed information\n================================================================================\n\n\n\n\nCode\n# Map Visualizations: Geographic Patterns\n# Top: ChatGPT Search Intensity\n# Bottom: 2x2 grid with all predictor variables\n\n# Install plotly if needed\nimport sys\nimport subprocess\ntry:\n    import plotly.express as px\n    import plotly.graph_objects as go\n    from plotly.subplots import make_subplots\nexcept ImportError:\n    print(\"Installing plotly...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"plotly\"])\n    import plotly.express as px\n    import plotly.graph_objects as go\n    from plotly.subplots import make_subplots\n    print(\"Plotly installed successfully!\")\n\n# Add state abbreviations for mapping\nstate_abbrev = {\n    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA',\n    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA',\n    'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO',\n    'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ',\n    'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH',\n    'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT',\n    'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY',\n    'District of Columbia': 'DC'\n}\n\n# Add state codes to merged_df if not already present\nif 'state_code' not in merged_df.columns:\n    merged_df['state_code'] = merged_df['State'].map(state_abbrev)\n\n# 1. TOP MAP: ChatGPT Search Intensity\nprint(\"=\" * 80)\nprint(\"MAP 1: ChatGPT Search Intensity\")\nprint(\"=\" * 80)\n\nfig_intensity = px.choropleth(\n    merged_df,\n    locations='state_code',\n    locationmode='USA-states',\n    color='Avg_AI_Interest',\n    scope='usa',\n    color_continuous_scale='Viridis',\n    title='ChatGPT Search Intensity by State',\n    labels={'Avg_AI_Interest': 'Search Intensity'},\n    hover_data=['State', 'Avg_AI_Interest']\n)\nfig_intensity.update_layout(\n    height=500,\n    title_x=0.5,\n    title_font_size=16,\n    font=dict(size=12)\n)\nfig_intensity.update_coloraxes(colorbar=dict(\n    thickness=8,\n    len=0.5,\n    x=1.01\n))\nfig_intensity.show()\n\nprint(\"\\n✓ ChatGPT Search Intensity map created\")\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MAP 2: Median Household Income\")\nprint(\"=\" * 80)\n\n# 2. Median Household Income\nfig_income = px.choropleth(\n    merged_df,\n    locations='state_code',\n    locationmode='USA-states',\n    color='median_household_income',\n    scope='usa',\n    color_continuous_scale='Blues',\n    title='Median Household Income by State',\n    labels={'median_household_income': 'Income ($)'},\n    hover_data=['State', 'median_household_income']\n)\nfig_income.update_layout(\n    height=500,\n    title_x=0.5,\n    title_font_size=16,\n    font=dict(size=12)\n)\nfig_income.update_coloraxes(colorbar=dict(\n    thickness=8,\n    len=0.5,\n    x=1.01\n))\nfig_income.show()\n\nprint(\"\\n✓ Median Household Income map created\")\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MAP 3: % Bachelor's Degree or Higher\")\nprint(\"=\" * 80)\n\n# 3. Education % Bachelor's+\nfig_education = px.choropleth(\n    merged_df,\n    locations='state_code',\n    locationmode='USA-states',\n    color='pct_bachelors_plus',\n    scope='usa',\n    color_continuous_scale='Greens',\n    title='% Adults with Bachelor\\'s Degree or Higher by State',\n    labels={'pct_bachelors_plus': '% Bachelor\\'s+'},\n    hover_data=['State', 'pct_bachelors_plus']\n)\nfig_education.update_layout(\n    height=500,\n    title_x=0.5,\n    title_font_size=16,\n    font=dict(size=12)\n)\nfig_education.update_coloraxes(colorbar=dict(\n    thickness=8,\n    len=0.5,\n    x=1.01\n))\nfig_education.show()\n\nprint(\"\\n✓ Education map created\")\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MAP 4: % Households with Internet Access\")\nprint(\"=\" * 80)\n\n# 4. Internet Access\nfig_internet = px.choropleth(\n    merged_df,\n    locations='state_code',\n    locationmode='USA-states',\n    color='pct_with_internet',\n    scope='usa',\n    color_continuous_scale='Oranges',\n    title='% Households with Internet Access by State',\n    labels={'pct_with_internet': '% With Internet'},\n    hover_data=['State', 'pct_with_internet']\n)\nfig_internet.update_layout(\n    height=500,\n    title_x=0.5,\n    title_font_size=16,\n    font=dict(size=12)\n)\nfig_internet.update_coloraxes(colorbar=dict(\n    thickness=8,\n    len=0.5,\n    x=1.01\n))\nfig_internet.show()\n\nprint(\"\\n✓ Internet Access map created\")\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MAP 5: % Knowledge Economy Employment\")\nprint(\"=\" * 80)\n\n# 5. Knowledge Economy\nfig_knowledge = px.choropleth(\n    merged_df,\n    locations='state_code',\n    locationmode='USA-states',\n    color='pct_knowledge',\n    scope='usa',\n    color_continuous_scale='Purples',\n    title='% Knowledge Economy Employment by State',\n    labels={'pct_knowledge': '% Knowledge Economy'},\n    hover_data=['State', 'pct_knowledge']\n)\nfig_knowledge.update_layout(\n    height=500,\n    title_x=0.5,\n    title_font_size=16,\n    font=dict(size=12)\n)\nfig_knowledge.update_coloraxes(colorbar=dict(\n    thickness=8,\n    len=0.5,\n    x=1.01\n))\nfig_knowledge.show()\n\nprint(\"\\n✓ Knowledge Economy map created\")\nprint(\"\\n✓ All predictor variable maps created\")\nprint(\"=\" * 80)\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MAP 6: Red/Blue States (2020 Presidential Election)\")\nprint(\"=\" * 80)\n\n# 6. Red/Blue States Map\n# Create a discrete color map: 0 = Red (Republican), 1 = Blue (Democratic)\n# Convert blue_state to categorical string for proper discrete mapping\nmerged_df_temp = merged_df.copy()\nmerged_df_temp['political_affiliation'] = merged_df_temp['blue_state'].map({0: 'Republican', 1: 'Democratic'})\n\nfig_political = px.choropleth(\n    merged_df_temp,\n    locations='state_code',\n    locationmode='USA-states',\n    color='political_affiliation',\n    scope='usa',\n    color_discrete_map={'Republican': '#E81B23', 'Democratic': '#0044C9'},  # Red for Republican, Blue for Democratic\n    title='Red States vs Blue States (2020 Presidential Election)',\n    labels={'political_affiliation': 'Political Affiliation'},\n    hover_data=['State', 'political_affiliation']\n)\nfig_political.update_layout(\n    height=500,\n    title_x=0.5,\n    title_font_size=16,\n    font=dict(size=12),\n    showlegend=True\n)\n# Update the traces for better visualization and remove colorbar\nfig_political.update_traces(\n    marker_line_width=0.5,\n    marker_line_color='white',\n    showscale=False  # Remove colorbar\n)\nfig_political.show()\n\nprint(\"\\n✓ Red/Blue States map created\")\n\n\n================================================================================\nMAP 1: ChatGPT Search Intensity\n================================================================================\n\n\n                                                    \n\n\n\n✓ ChatGPT Search Intensity map created\n\n================================================================================\nMAP 2: Median Household Income\n================================================================================\n\n\n                                                    \n\n\n\n✓ Median Household Income map created\n\n================================================================================\nMAP 3: % Bachelor's Degree or Higher\n================================================================================\n\n\n                                                    \n\n\n\n✓ Education map created\n\n================================================================================\nMAP 4: % Households with Internet Access\n================================================================================\n\n\n                                                    \n\n\n\n✓ Internet Access map created\n\n================================================================================\nMAP 5: % Knowledge Economy Employment\n================================================================================\n\n\n                                                    \n\n\n\n✓ Knowledge Economy map created\n\n✓ All predictor variable maps created\n================================================================================\n\n================================================================================\nMAP 6: Red/Blue States (2020 Presidential Election)\n================================================================================\n\n\n                                                    \n\n\n\n✓ Red/Blue States map created"
  },
  {
    "objectID": "Final_Project_Sam_Sen.html#regression-analysis",
    "href": "Final_Project_Sam_Sen.html#regression-analysis",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "8. Regression Analysis",
    "text": "8. Regression Analysis\n\n\nCode\n# Multiple Linear Regression: All Variables Together\n# Dependent variable: Avg_AI_Interest (ChatGPT search intensity)\n# Independent variables: Income, Education, Internet, Knowledge Economy, Blue State\n\n# Install statsmodels if needed\nimport sys\nimport subprocess\ntry:\n    import statsmodels.api as sm\nexcept ImportError:\n    print(\"Installing statsmodels...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"statsmodels\"])\n    import statsmodels.api as sm\n\n# Prepare data - remove rows with any missing values\nregression_data = merged_df[['Avg_AI_Interest', 'median_household_income', \n                              'pct_bachelors_plus', 'pct_with_internet', 'pct_knowledge', 'blue_state']].dropna()\n\nprint(\"=\" * 80)\nprint(\"MULTIPLE LINEAR REGRESSION: ALL VARIABLES\")\nprint(\"=\" * 80)\nprint(f\"\\nObservations: {len(regression_data)}\")\nprint(f\"\\nVariables included in the model:\")\nprint(f\"  1. Median Household Income\")\nprint(f\"  2. % Bachelor's Degree or Higher (pct_bachelors_plus)\")\nprint(f\"  3. % Households with Internet Access (pct_with_internet)\")\nprint(f\"  4. % Knowledge Economy Employment (pct_knowledge)\")\nprint(f\"  5. Blue State (vs Red State) - Dummy variable (blue_state)\\n\")\nprint(f\"Dependent Variable: ChatGPT Search Intensity (Avg_AI_Interest)\\n\")\n\n# Prepare X and y\nX = regression_data[['median_household_income', 'pct_bachelors_plus', \n                     'pct_with_internet', 'pct_knowledge', 'blue_state']]\ny = regression_data['Avg_AI_Interest']\n\n# Add constant term for intercept\nX_with_const = sm.add_constant(X)\n\n# Fit the model\nmodel_all = sm.OLS(y, X_with_const).fit()\n\n# Verify all variables are included\nprint(f\"\\n✓ Model fitted successfully!\")\nprint(f\"  Number of predictors: {len(X.columns)}\")\nprint(f\"  Predictor names: {list(X.columns)}\")\nprint(f\"  Total parameters (including intercept): {len(model_all.params)}\")\nprint(f\"  Model degrees of freedom: {model_all.df_model}\")\n\n# Display model summary\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MODEL SUMMARY\")\nprint(\"=\" * 80)\nprint(\"\\nFull regression results with all 5 predictor variables:\\n\")\nprint(model_all.summary())\nprint(\"\\n\" + \"=\" * 80)\n\n# Create coefficients table with full details\n# Add significance markers\nvar_names = ['Intercept', 'Median Household Income', '% Bachelor\\'s+', \n             '% With Internet', '% Knowledge Economy', 'Blue State (vs Red)']\nsignificance_markers = []\nfor pval in model_all.pvalues:\n    if pval &lt; 0.001:\n        significance_markers.append('***')\n    elif pval &lt; 0.01:\n        significance_markers.append('**')\n    elif pval &lt; 0.05:\n        significance_markers.append('*')\n    else:\n        significance_markers.append('')\n\n# Add significance to variable names\nvar_names_with_sig = [f\"{var} {sig}\" if sig else var for var, sig in zip(var_names, significance_markers)]\n\ncoef_table = pd.DataFrame({\n    'Variable': var_names_with_sig,\n    'Coefficient': model_all.params.values,\n    'Std Error': model_all.bse.values,\n    't-value': model_all.tvalues.values,\n    'P-value': model_all.pvalues.values,\n    'Significant': ['Yes' if p &lt; 0.05 else 'No' for p in model_all.pvalues],\n    'Lower CI (95%)': model_all.conf_int()[0].values,\n    'Upper CI (95%)': model_all.conf_int()[1].values\n})\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"COEFFICIENTS TABLE (COMPLETE)\")\nprint(\"=\" * 80)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\nprint(\"\\n\" + coef_table.to_string(index=False))\nprint(\"\\n\" + \"=\" * 80)\n\n# Model interpretation\nprint(f\"\\n\" + \"=\" * 80)\nprint(\"MODEL INTERPRETATION\")\nprint(\"=\" * 80)\nprint(f\"\\nModel Fit:\")\nprint(f\"  • R-squared: {model_all.rsquared:.4f} ({model_all.rsquared:.1%})\")\nprint(f\"  • Adjusted R-squared: {model_all.rsquared_adj:.4f} ({model_all.rsquared_adj:.1%})\")\nprint(f\"  • F-statistic: {model_all.fvalue:.4f}\")\nprint(f\"  • F-statistic p-value: {model_all.f_pvalue:.6f}\")\nif model_all.f_pvalue &lt; 0.05:\n    print(\"  ✓ Model is statistically significant overall\")\nelse:\n    print(\"  ✗ Model is not statistically significant overall\")\n\nprint(f\"\\n\" + \"-\" * 80)\nprint(\"COEFFICIENT INTERPRETATION:\")\nprint(\"-\" * 80)\n\n# Create a summary of significant variables\nsignificant_vars = []\nnon_significant_vars = []\n\nfor i, (var, coef, pval) in enumerate(zip(['Intercept', 'Median Household Income', '% Bachelor\\'s+', \n                                           '% With Internet', '% Knowledge Economy', 'Blue State (vs Red)'], \n                                          model_all.params, model_all.pvalues)):\n    significance = \"***\" if pval &lt; 0.001 else \"**\" if pval &lt; 0.01 else \"*\" if pval &lt; 0.05 else \"\"\n    print(f\"\\n{var}:\")\n    print(f\"  Coefficient: {coef:.6f}\")\n    print(f\"  P-value: {pval:.6f} {significance}\")\n    if i &gt; 0:  # Skip intercept\n        if pval &lt; 0.05:\n            significant_vars.append((var, coef, pval, significance))\n            if var == 'Blue State (vs Red)':\n                direction = \"higher\" if coef &gt; 0 else \"lower\"\n                print(f\"  Interpretation: ✓ Statistically significant - Blue states have {direction} ChatGPT search intensity than red states (by {abs(coef):.2f} points)\")\n            else:\n                direction = \"increases\" if coef &gt; 0 else \"decreases\"\n                print(f\"  Interpretation: ✓ Statistically significant - {direction} ChatGPT search intensity\")\n        else:\n            non_significant_vars.append((var, coef, pval))\n            print(f\"  Interpretation: ✗ Not statistically significant (p = {pval:.4f})\")\n\n# Summary of significant variables\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SUMMARY: SIGNIFICANT PREDICTORS\")\nprint(\"=\" * 80)\nif significant_vars:\n    print(f\"\\n✓ {len(significant_vars)} variable(s) are statistically significant:\")\n    for var, coef, pval, sig in significant_vars:\n        print(f\"  • {var}: p = {pval:.4f} {sig}\")\nelse:\n    print(\"\\n⚠ No individual predictors are statistically significant at p &lt; 0.05\")\n\nif non_significant_vars:\n    print(f\"\\n✗ {len(non_significant_vars)} variable(s) are NOT statistically significant:\")\n    for var, coef, pval in non_significant_vars:\n        print(f\"  • {var}: p = {pval:.4f}\")\n\nprint(\"\\n\" + \"-\" * 80)\nprint(\"NOTE ON MULTICOLLINEARITY:\")\nprint(\"-\" * 80)\nprint(\"When variables are highly correlated (multicollinearity), individual predictors\")\nprint(\"may not be significant even though the model overall is significant.\")\nprint(\"This is because the variables share predictive power, making it difficult to\")\nprint(\"separate their individual effects. Check VIF values to assess multicollinearity.\")\nprint(\"=\" * 80)\n\n\n================================================================================\nMULTIPLE LINEAR REGRESSION: ALL VARIABLES\n================================================================================\n\nObservations: 51\n\nVariables included in the model:\n  1. Median Household Income\n  2. % Bachelor's Degree or Higher (pct_bachelors_plus)\n  3. % Households with Internet Access (pct_with_internet)\n  4. % Knowledge Economy Employment (pct_knowledge)\n  5. Blue State (vs Red State) - Dummy variable (blue_state)\n\nDependent Variable: ChatGPT Search Intensity (Avg_AI_Interest)\n\n\n✓ Model fitted successfully!\n  Number of predictors: 5\n  Predictor names: ['median_household_income', 'pct_bachelors_plus', 'pct_with_internet', 'pct_knowledge', 'blue_state']\n  Total parameters (including intercept): 6\n  Model degrees of freedom: 5.0\n\n================================================================================\nMODEL SUMMARY\n================================================================================\n\nFull regression results with all 5 predictor variables:\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:        Avg_AI_Interest   R-squared:                       0.387\nModel:                            OLS   Adj. R-squared:                  0.319\nMethod:                 Least Squares   F-statistic:                     5.683\nDate:                Sat, 06 Dec 2025   Prob (F-statistic):           0.000382\nTime:                        23:28:09   Log-Likelihood:                -185.18\nNo. Observations:                  51   AIC:                             382.4\nDf Residuals:                      45   BIC:                             393.9\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n===========================================================================================\n                              coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------------\nconst                      47.4779     70.790      0.671      0.506     -95.100     190.056\nmedian_household_income     0.0003      0.000      1.169      0.248      -0.000       0.001\npct_bachelors_plus          0.0595      0.422      0.141      0.889      -0.791       0.910\npct_with_internet          -0.4460      0.956     -0.466      0.643      -2.372       1.480\npct_knowledge               0.9469      0.403      2.348      0.023       0.135       1.759\nblue_state                 -0.8240      3.904     -0.211      0.834      -8.688       7.040\n==============================================================================\nOmnibus:                        0.980   Durbin-Watson:                   0.784\nProb(Omnibus):                  0.613   Jarque-Bera (JB):                1.016\nSkew:                           0.301   Prob(JB):                        0.602\nKurtosis:                       2.660   Cond. No.                     3.94e+06\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.94e+06. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n================================================================================\n\n================================================================================\nCOEFFICIENTS TABLE (COMPLETE)\n================================================================================\n\n               Variable  Coefficient  Std Error   t-value  P-value Significant  Lower CI (95%)  Upper CI (95%)\n              Intercept    47.477917  70.789798  0.670689 0.505847          No      -95.100056      190.055889\nMedian Household Income     0.000350   0.000299  1.169385 0.248406          No       -0.000253        0.000953\n          % Bachelor's+     0.059543   0.422372  0.140973 0.888520          No       -0.791158        0.910245\n        % With Internet    -0.445992   0.956123 -0.466459 0.643138          No       -2.371723        1.479739\n  % Knowledge Economy *     0.946938   0.403272  2.348135 0.023323         Yes        0.134706        1.759170\n    Blue State (vs Red)    -0.824011   3.904356 -0.211049 0.833802          No       -8.687788        7.039766\n\n================================================================================\n\n================================================================================\nMODEL INTERPRETATION\n================================================================================\n\nModel Fit:\n  • R-squared: 0.3870 (38.7%)\n  • Adjusted R-squared: 0.3189 (31.9%)\n  • F-statistic: 5.6825\n  • F-statistic p-value: 0.000382\n  ✓ Model is statistically significant overall\n\n--------------------------------------------------------------------------------\nCOEFFICIENT INTERPRETATION:\n--------------------------------------------------------------------------------\n\nIntercept:\n  Coefficient: 47.477917\n  P-value: 0.505847 \n\nMedian Household Income:\n  Coefficient: 0.000350\n  P-value: 0.248406 \n  Interpretation: ✗ Not statistically significant (p = 0.2484)\n\n% Bachelor's+:\n  Coefficient: 0.059543\n  P-value: 0.888520 \n  Interpretation: ✗ Not statistically significant (p = 0.8885)\n\n% With Internet:\n  Coefficient: -0.445992\n  P-value: 0.643138 \n  Interpretation: ✗ Not statistically significant (p = 0.6431)\n\n% Knowledge Economy:\n  Coefficient: 0.946938\n  P-value: 0.023323 *\n  Interpretation: ✓ Statistically significant - increases ChatGPT search intensity\n\nBlue State (vs Red):\n  Coefficient: -0.824011\n  P-value: 0.833802 \n  Interpretation: ✗ Not statistically significant (p = 0.8338)\n\n================================================================================\nSUMMARY: SIGNIFICANT PREDICTORS\n================================================================================\n\n✓ 1 variable(s) are statistically significant:\n  • % Knowledge Economy: p = 0.0233 *\n\n✗ 4 variable(s) are NOT statistically significant:\n  • Median Household Income: p = 0.2484\n  • % Bachelor's+: p = 0.8885\n  • % With Internet: p = 0.6431\n  • Blue State (vs Red): p = 0.8338\n\n--------------------------------------------------------------------------------\nNOTE ON MULTICOLLINEARITY:\n--------------------------------------------------------------------------------\nWhen variables are highly correlated (multicollinearity), individual predictors\nmay not be significant even though the model overall is significant.\nThis is because the variables share predictive power, making it difficult to\nseparate their individual effects. Check VIF values to assess multicollinearity.\n================================================================================\n\n\n\n\nCode\n# Multicollinearity Check: Variance Inflation Factor (VIF)\n# VIF &gt; 10 indicates potential multicollinearity issues\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nprint(\"=\" * 80)\nprint(\"MULTICOLLINEARITY DIAGNOSTICS (VIF)\")\nprint(\"=\" * 80)\n\n# Use X from the full model (should be defined from previous cell)\n# If not, prepare it\nif 'X' not in locals():\n    if 'regression_data' not in locals() or (isinstance(regression_data, pd.DataFrame) and regression_data.empty):\n        regression_data = merged_df[['Avg_AI_Interest', 'median_household_income', \n                                      'pct_bachelors_plus', 'pct_with_internet', 'pct_knowledge', 'blue_state']].dropna()\n    X = regression_data[['median_household_income', 'pct_bachelors_plus', \n                         'pct_with_internet', 'pct_knowledge', 'blue_state']]\n\n# Calculate VIF for each predictor variable\n# Note: We use X (without constant) for VIF calculation\nvif_data = pd.DataFrame()\nvif_data[\"Variable\"] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\nprint(\"\\nVariance Inflation Factors:\")\nprint(vif_data.to_string(index=False))\n\nprint(\"\\n\" + \"-\" * 80)\nprint(\"Interpretation:\")\nprint(\"-\" * 80)\nprint(\"- VIF &lt; 5: No multicollinearity concern\")\nprint(\"- VIF 5-10: Moderate multicollinearity\")\nprint(\"- VIF &gt; 10: High multicollinearity (may need to address)\")\n\nhigh_vif = vif_data[vif_data['VIF'] &gt; 10]\nif len(high_vif) &gt; 0:\n    print(f\"\\n⚠ Warning: {len(high_vif)} variable(s) with VIF &gt; 10:\")\n    print(high_vif.to_string(index=False))\n    print(\"\\n⚠⚠⚠ MULTICOLLINEARITY DETECTED ⚠⚠⚠\")\n    print(\"High VIF values indicate that the predictors are highly correlated with each other,\")\n    print(\"making it difficult to separate their individual effects.\")\n    print(\"\\nThis explains why:\")\n    print(\"  • The model is significant overall but individual predictors may not be\")\n    print(\"  • The condition number is very high\")\n    print(\"  • Coefficient estimates may be unstable\")\n    print(\"\\nRecommendation: Consider using individual regressions or removing highly correlated variables\")\nelse:\n    print(\"\\n✓ No severe multicollinearity detected (all VIF &lt; 10)\")\n\nprint(\"=\" * 80)\n\n\n================================================================================\nMULTICOLLINEARITY DIAGNOSTICS (VIF)\n================================================================================\n\nVariance Inflation Factors:\n               Variable        VIF\nmedian_household_income 163.747450\n     pct_bachelors_plus 108.726848\n      pct_with_internet 147.544144\n          pct_knowledge 120.113120\n             blue_state   4.184271\n\n--------------------------------------------------------------------------------\nInterpretation:\n--------------------------------------------------------------------------------\n- VIF &lt; 5: No multicollinearity concern\n- VIF 5-10: Moderate multicollinearity\n- VIF &gt; 10: High multicollinearity (may need to address)\n\n⚠ Warning: 4 variable(s) with VIF &gt; 10:\n               Variable        VIF\nmedian_household_income 163.747450\n     pct_bachelors_plus 108.726848\n      pct_with_internet 147.544144\n          pct_knowledge 120.113120\n\n⚠⚠⚠ MULTICOLLINEARITY DETECTED ⚠⚠⚠\nHigh VIF values indicate that the predictors are highly correlated with each other,\nmaking it difficult to separate their individual effects.\n\nThis explains why:\n  • The model is significant overall but individual predictors may not be\n  • The condition number is very high\n  • Coefficient estimates may be unstable\n\nRecommendation: Consider using individual regressions or removing highly correlated variables\n================================================================================\n\n\n\n\nCode\n# Regression with Knowledge and Blue State Only\n# Simple two-predictor model: pct_knowledge and blue_state\n\nprint(\"=\" * 80)\nprint(\"REGRESSION: KNOWLEDGE + BLUE STATE ONLY\")\nprint(\"=\" * 80)\n\n# Prepare data - remove rows with any missing values for these specific variables\nregression_data_simple = merged_df[['Avg_AI_Interest', 'pct_knowledge', 'blue_state']].dropna()\n\nprint(f\"\\nObservations: {len(regression_data_simple)}\")\nprint(f\"\\nVariables included in the model:\")\nprint(f\"  1. % Knowledge Economy Employment (pct_knowledge)\")\nprint(f\"  2. Blue State (vs Red State) - Dummy variable (blue_state)\")\nprint(f\"\\nDependent Variable: ChatGPT Search Intensity (Avg_AI_Interest)\\n\")\n\n# Prepare X and y\nX_simple = regression_data_simple[['pct_knowledge', 'blue_state']]\ny_simple = regression_data_simple['Avg_AI_Interest']\n\n# Add constant term for intercept\nX_simple_const = sm.add_constant(X_simple)\n\n# Fit the model\nmodel_simple = sm.OLS(y_simple, X_simple_const).fit()\n\nprint(f\"✓ Model fitted successfully!\")\nprint(f\"  Number of predictors: {len(X_simple.columns)}\")\nprint(f\"  Predictor names: {list(X_simple.columns)}\")\n\n# Display model summary\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MODEL SUMMARY\")\nprint(\"=\" * 80)\nprint(\"\\nRegression results with Knowledge and Blue State:\\n\")\nprint(model_simple.summary())\nprint(\"\\n\" + \"=\" * 80)\n\n# Create coefficients table\nvar_names = ['Intercept', '% Knowledge Economy', 'Blue State (vs Red)']\nsignificance_markers = []\nfor pval in model_simple.pvalues:\n    if pval &lt; 0.001:\n        significance_markers.append('***')\n    elif pval &lt; 0.01:\n        significance_markers.append('**')\n    elif pval &lt; 0.05:\n        significance_markers.append('*')\n    else:\n        significance_markers.append('')\n\nvar_names_with_sig = [f\"{var} {sig}\" if sig else var for var, sig in zip(var_names, significance_markers)]\n\ncoef_table_simple = pd.DataFrame({\n    'Variable': var_names_with_sig,\n    'Coefficient': model_simple.params.values,\n    'Std Error': model_simple.bse.values,\n    't-value': model_simple.tvalues.values,\n    'P-value': model_simple.pvalues.values,\n    'Significant': ['Yes' if p &lt; 0.05 else 'No' for p in model_simple.pvalues],\n    'Lower CI (95%)': model_simple.conf_int()[0].values,\n    'Upper CI (95%)': model_simple.conf_int()[1].values\n})\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"COEFFICIENTS TABLE\")\nprint(\"=\" * 80)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\nprint(\"\\n\" + coef_table_simple.to_string(index=False))\nprint(\"\\n\" + \"=\" * 80)\n\n# Model interpretation\nprint(f\"\\n\" + \"=\" * 80)\nprint(\"MODEL INTERPRETATION\")\nprint(\"=\" * 80)\nprint(f\"\\nModel Fit:\")\nprint(f\"  • R-squared: {model_simple.rsquared:.4f} ({model_simple.rsquared:.1%})\")\nprint(f\"  • Adjusted R-squared: {model_simple.rsquared_adj:.4f} ({model_simple.rsquared_adj:.1%})\")\nprint(f\"  • F-statistic: {model_simple.fvalue:.4f}\")\nprint(f\"  • F-statistic p-value: {model_simple.f_pvalue:.6f}\")\nif model_simple.f_pvalue &lt; 0.05:\n    print(\"  ✓ Model is statistically significant overall\")\nelse:\n    print(\"  ✗ Model is not statistically significant overall\")\n\nprint(f\"\\n\" + \"-\" * 80)\nprint(\"COEFFICIENT INTERPRETATION:\")\nprint(\"-\" * 80)\n\n# Interpret each coefficient\nfor i, (var, coef, pval) in enumerate(zip(['Intercept', '% Knowledge Economy', 'Blue State (vs Red)'], \n                                          model_simple.params, model_simple.pvalues)):\n    significance = \"***\" if pval &lt; 0.001 else \"**\" if pval &lt; 0.01 else \"*\" if pval &lt; 0.05 else \"\"\n    print(f\"\\n{var}:\")\n    print(f\"  Coefficient: {coef:.6f}\")\n    print(f\"  P-value: {pval:.6f} {significance}\")\n    if i == 1:  # Knowledge Economy\n        if pval &lt; 0.05:\n            direction = \"increases\" if coef &gt; 0 else \"decreases\"\n            print(f\"  Interpretation: ✓ Statistically significant - A 1 percentage point increase in knowledge economy employment {direction} ChatGPT search intensity by {abs(coef):.2f} points\")\n        else:\n            print(f\"  Interpretation: ✗ Not statistically significant (p = {pval:.4f})\")\n    elif i == 2:  # Blue State\n        if pval &lt; 0.05:\n            direction = \"higher\" if coef &gt; 0 else \"lower\"\n            print(f\"  Interpretation: ✓ Statistically significant - Blue states have {direction} ChatGPT search intensity than red states (by {abs(coef):.2f} points)\")\n        else:\n            print(f\"  Interpretation: ✗ Not statistically significant (p = {pval:.4f})\")\n\n# Check VIF for this simpler model\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MULTICOLLINEARITY CHECK (VIF)\")\nprint(\"=\" * 80)\nvif_simple = pd.DataFrame()\nvif_simple[\"Variable\"] = X_simple.columns\nvif_simple[\"VIF\"] = [variance_inflation_factor(X_simple.values, i) for i in range(X_simple.shape[1])]\nprint(\"\\n\" + vif_simple.to_string(index=False))\nhigh_vif_simple = vif_simple[vif_simple['VIF'] &gt; 10]\nif len(high_vif_simple) &gt; 0:\n    print(f\"\\n⚠ Warning: {len(high_vif_simple)} variable(s) with VIF &gt; 10\")\nelse:\n    print(\"\\n✓ No multicollinearity concerns (all VIF &lt; 10)\")\n\nprint(\"=\" * 80)\n\n\n================================================================================\nREGRESSION: KNOWLEDGE + BLUE STATE ONLY\n================================================================================\n\nObservations: 51\n\nVariables included in the model:\n  1. % Knowledge Economy Employment (pct_knowledge)\n  2. Blue State (vs Red State) - Dummy variable (blue_state)\n\nDependent Variable: ChatGPT Search Intensity (Avg_AI_Interest)\n\n✓ Model fitted successfully!\n  Number of predictors: 2\n  Predictor names: ['pct_knowledge', 'blue_state']\n\n================================================================================\nMODEL SUMMARY\n================================================================================\n\nRegression results with Knowledge and Blue State:\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:        Avg_AI_Interest   R-squared:                       0.329\nModel:                            OLS   Adj. R-squared:                  0.301\nMethod:                 Least Squares   F-statistic:                     11.76\nDate:                Sat, 06 Dec 2025   Prob (F-statistic):           6.97e-05\nTime:                        23:28:09   Log-Likelihood:                -187.49\nNo. Observations:                  51   AIC:                             381.0\nDf Residuals:                      48   BIC:                             386.8\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst            27.4584     11.926      2.302      0.026       3.480      51.437\npct_knowledge     1.1454      0.352      3.250      0.002       0.437       1.854\nblue_state        2.4993      3.585      0.697      0.489      -4.709       9.707\n==============================================================================\nOmnibus:                        0.295   Durbin-Watson:                   0.667\nProb(Omnibus):                  0.863   Jarque-Bera (JB):                0.443\nSkew:                           0.152   Prob(JB):                        0.801\nKurtosis:                       2.660   Cond. No.                         325.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n================================================================================\n\n================================================================================\nCOEFFICIENTS TABLE\n================================================================================\n\n              Variable  Coefficient  Std Error  t-value  P-value Significant  Lower CI (95%)  Upper CI (95%)\n           Intercept *    27.458393  11.925721 2.302451 0.025689         Yes        3.480123       51.436663\n% Knowledge Economy **     1.145363   0.352459 3.249632 0.002114         Yes        0.436696        1.854031\n   Blue State (vs Red)     2.499283   3.584887 0.697172 0.489058          No       -4.708616        9.707182\n\n================================================================================\n\n================================================================================\nMODEL INTERPRETATION\n================================================================================\n\nModel Fit:\n  • R-squared: 0.3289 (32.9%)\n  • Adjusted R-squared: 0.3009 (30.1%)\n  • F-statistic: 11.7613\n  • F-statistic p-value: 0.000070\n  ✓ Model is statistically significant overall\n\n--------------------------------------------------------------------------------\nCOEFFICIENT INTERPRETATION:\n--------------------------------------------------------------------------------\n\nIntercept:\n  Coefficient: 27.458393\n  P-value: 0.025689 *\n\n% Knowledge Economy:\n  Coefficient: 1.145363\n  P-value: 0.002114 **\n  Interpretation: ✓ Statistically significant - A 1 percentage point increase in knowledge economy employment increases ChatGPT search intensity by 1.15 points\n\nBlue State (vs Red):\n  Coefficient: 2.499283\n  P-value: 0.489058 \n  Interpretation: ✗ Not statistically significant (p = 0.4891)\n\n================================================================================\nMULTICOLLINEARITY CHECK (VIF)\n================================================================================\n\n     Variable      VIF\npct_knowledge 2.443524\n   blue_state 2.443524\n\n✓ No multicollinearity concerns (all VIF &lt; 10)\n================================================================================"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "",
    "text": "How is search intensity for AI (specifically ChatGPT) associated with income, education, broadband access, and industry composition at the state level?\nThis analysis examines correlations between: - ChatGPT search intensity (Google Trends) - Median household income - Education levels (% with bachelor’s degree or higher) - Internet access (% households with internet) - Knowledge economy employment (% in knowledge industries)"
  },
  {
    "objectID": "index.html#research-question",
    "href": "index.html#research-question",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "",
    "text": "How is search intensity for AI (specifically ChatGPT) associated with income, education, broadband access, and industry composition at the state level?\nThis analysis examines correlations between: - ChatGPT search intensity (Google Trends) - Median household income - Education levels (% with bachelor’s degree or higher) - Internet access (% households with internet) - Knowledge economy employment (% in knowledge industries)"
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "Project Overview",
    "text": "Project Overview\nThis project analyzes state-level patterns in AI search interest using multiple regression analysis to understand the socioeconomic factors associated with ChatGPT search intensity across the United States. The analysis uses data from 51 observations (50 states + District of Columbia) to identify key predictors of AI technology adoption and interest."
  },
  {
    "objectID": "index.html#methodology",
    "href": "index.html#methodology",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "Methodology",
    "text": "Methodology\n\nData Collection\n\nGoogle Trends: State-level search interest for “ChatGPT” (past 12 months)\nAmerican Community Survey (ACS) 5-year estimates (2022):\n\nTable B19013: Median household income\nTable B15003: Educational attainment (% bachelor’s+)\nTable B28002: Internet subscription (% with internet)\n\nBLS Industry Data: Knowledge economy employment percentages\n2020 Presidential Election Results: Political affiliation (blue/red state dummy variable)\n\n\n\nAnalytical Approach\n\nCorrelation analysis to identify relationships\nMultiple linear regression (OLS) with all predictors\nRidge and Lasso regression to address multicollinearity\nCross-validation to assess model performance\nGeographic visualizations (choropleth maps)"
  },
  {
    "objectID": "index.html#key-findings",
    "href": "index.html#key-findings",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "Key Findings",
    "text": "Key Findings\n\nCorrelations\n\nStrong positive correlations between ChatGPT search intensity and:\n\nKnowledge economy employment (r = 0.568)\nEducation levels (r = 0.530)\nMedian household income (r = 0.518)\nPolitical affiliation - Blue states (r = 0.426)\nInternet access (r = 0.404)\n\n\n\n\nRegression Results\n\nModel R² = 0.387 (38.7% of variance explained)\nOverall model significance: p &lt; 0.001\nSignificant predictor: Knowledge economy employment (p = 0.023)\nMulticollinearity: High correlation between predictors (VIF &gt; 20 for most variables)\n\n\n\nGeographic Patterns\nHigher ChatGPT search intensity is observed in states with: - Higher education levels - Higher median income - Better internet infrastructure - Larger knowledge economy sectors - Democratic political affiliation (blue states)"
  },
  {
    "objectID": "index.html#limitations",
    "href": "index.html#limitations",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "Limitations",
    "text": "Limitations\n\nData Limitations\n\nSample Size: 51 observations (states + DC) limits statistical power\nTemporal Scope: Analysis uses 12-month Google Trends data; longer time series could reveal trends\nGoogle Trends Data:\n\nRelative search intensity (0-100 scale) rather than absolute search volume\nNote on Metro/City-Level Data: Google Trends data at metro or city levels is less reliable due to smaller sample sizes, privacy thresholds that suppress low-volume searches, and inconsistent geographic boundaries. State-level aggregation provides more stable and reliable estimates.\n\nCross-Sectional Analysis: Cannot establish causality; only associations are identified\nMulticollinearity: High correlation between socioeconomic variables makes it difficult to isolate individual effects\n\n\n\nMethodological Limitations\n\nMissing Variables: Other factors (e.g., tech industry presence, research institutions, media coverage) not included\nLinear Assumptions: Assumes linear relationships; non-linear patterns may exist\nHomogeneity: Assumes relationships are consistent across all states"
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "Next Steps",
    "text": "Next Steps\n\nFuture Research Directions\n\nTemporal Analysis: Examine trends over multiple years to identify changes in AI interest\nCausal Inference: Use instrumental variables or natural experiments to establish causality\nAdditional Predictors: Include variables such as:\n\nTech industry employment\nUniversity research funding\nMedia coverage of AI\nState-level AI policies\n\nSub-State Analysis: Explore county or metro-level patterns where data quality permits\nComparative Analysis: Compare ChatGPT interest with other AI tools (Gemini, Claude)\nQualitative Research: Conduct interviews or surveys to understand motivations behind searches\n\n\n\nMethodological Improvements\n\nAddress Multicollinearity: Use principal component analysis or factor analysis\nNon-Linear Models: Explore polynomial or interaction terms\nSpatial Analysis: Account for spatial autocorrelation between neighboring states\nPanel Data: Collect multi-year data for longitudinal analysis"
  },
  {
    "objectID": "index.html#conclusions",
    "href": "index.html#conclusions",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "Conclusions",
    "text": "Conclusions\nThis analysis reveals that ChatGPT search intensity is significantly associated with socioeconomic factors, particularly knowledge economy employment. The findings suggest that states with stronger knowledge-based economies and higher education levels show greater interest in AI technologies like ChatGPT.\nWhile the model explains a moderate portion of variance (38.7%), the presence of multicollinearity indicates that these socioeconomic factors are highly interrelated. The knowledge economy variable emerges as the most significant individual predictor, highlighting the role of economic structure in technology adoption.\nThese results have implications for understanding the digital divide in AI technology awareness and adoption, suggesting that educational and economic development policies may influence how communities engage with emerging AI tools."
  },
  {
    "objectID": "index.html#full-analysis",
    "href": "index.html#full-analysis",
    "title": "State-Level ChatGPT Search Intensity Analysis",
    "section": "Full Analysis",
    "text": "Full Analysis\nView the complete analysis with all visualizations, regression results, and detailed methodology →\n\nProject Information\n\nCourse: MUSA 5500 Final Project\nAnalysis Period: December 2024\nData Sources: Google Trends, U.S. Census Bureau (ACS), Bureau of Labor Statistics\nLast Updated: December 2024"
  }
]